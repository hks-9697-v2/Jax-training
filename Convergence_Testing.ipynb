{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hks-9697-v2/Jax-training/blob/main/Convergence_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sq6YucvYtZ2H"
      },
      "outputs": [],
      "source": [
        "#https://arxiv.org/pdf/2301.05217v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m_psUADCuR3k"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import flax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6dqYuG1uVZg",
        "outputId": "fa01e5bf-b5a0-4d89-c087-a4364d409289"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[CpuDevice(id=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "jax.devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RDuGcCbxu-fz"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "# User input: The model takes 2 numbers as input 0 to 100 and get one number as output.\n",
        "class Grok(nn.Module):\n",
        "  n_attention_heads: int\n",
        "  d_dense_layers: int\n",
        "  x_dense_layer_size: int\n",
        "  num_embeddings: int = 101 # For numbers 1 to 100 (index 0 to 100)\n",
        "  embed_dim: int = 64 # Default embedding dimension\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    # x is expected to be a batch of two numbers (indices), e.g., (batch_size, 2)\n",
        "    # 1. Apply embedding layer to the input x\n",
        "    # The embedding layer will look up embeddings for each index in the input x\n",
        "    # x.shape = (batch_size, 2)\n",
        "    # embedded_x.shape = (batch_size, 2, embed_dim)\n",
        "    embedded_x = self.embedding_layer(x)\n",
        "\n",
        "    # For attention, we typically need a single sequence, or process elements sequentially.\n",
        "    # Given 'x' is a batch of two numbers, we can treat them as two tokens in a sequence\n",
        "    # and apply attention to allow them to interact.\n",
        "    # The attention block usually expects (batch, sequence_length, features).\n",
        "    # Here, sequence_length is 2.\n",
        "\n",
        "    # 2. Pass the embedded output through the attention block\n",
        "    # MultiHeadDotProductAttention expects query, key, value. For self-attention, all are the same.\n",
        "    # Output of attention block will have the same shape as input: (batch_size, 2, embed_dim)\n",
        "    attended_x = self.attention_block(inputs_q=embedded_x, inputs_kv=embedded_x)\n",
        "\n",
        "    # After attention, we might want to aggregate or process further. Let's assume\n",
        "    # we flatten the last two dimensions (sequence_length and embed_dim) for the dense layers\n",
        "    # or perhaps take the mean/sum across the sequence length if a single output vector per batch is desired.\n",
        "    # For now, let's assume we want to apply dense layers to each embedded token independently after attention,\n",
        "    # or reshape to handle the sequence. A common pattern is to apply a global pooling or reshape.\n",
        "    # Let's flatten for simplicity to feed into the first dense layer as a single feature vector per batch item,\n",
        "    # or treat the attended tokens as separate features for a combined dense layer processing.\n",
        "    # For this task, let's treat the output of the attention block as a combined feature for the dense layers.\n",
        "    # Reshape from (batch_size, 2, embed_dim) to (batch_size, 2 * embed_dim) or apply dense layers per token.\n",
        "    # Let's apply dense layers to the averaged representation of the two tokens for simplicity.\n",
        "    # Averaging along the sequence dimension (axis=1) to get (batch_size, embed_dim)\n",
        "    processed_x = jnp.mean(attended_x, axis=1) # (batch_size, embed_dim)\n",
        "\n",
        "    # 3. Iterate through self.dense_layers, applying each dense layer with ReLU activation\n",
        "    for i, dense_layer in enumerate(self.dense_layers):\n",
        "      processed_x = dense_layer(processed_x)\n",
        "      # Apply ReLU activation, excluding the very last dense layer if desired, but typically included.\n",
        "      if i < len(self.dense_layers) - 1 or self.d_dense_layers > 0: # Always apply if there are layers\n",
        "        processed_x = nn.relu(processed_x)\n",
        "\n",
        "    # 4. Pass the result through the unembedding layer\n",
        "    # The unembedding layer projects back to num_embeddings for prediction (e.g., next number token).\n",
        "    logits = self.unembedding_layer(processed_x)\n",
        "\n",
        "    return logits\n",
        "\n",
        "  def setup(self):\n",
        "    # a. Define an embedding layer\n",
        "    self.embedding_layer = nn.Embed(num_embeddings=self.num_embeddings, features=self.embed_dim)\n",
        "\n",
        "    # b. Define the attention mechanism (MultiHeadDotProductAttention)\n",
        "    # Set num_heads to self.n_attention_heads\n",
        "    # Set qkv_features and out_features to self.embed_dim\n",
        "    self.attention_block = nn.MultiHeadDotProductAttention(\n",
        "        num_heads=self.n_attention_heads,\n",
        "        qkv_features=self.embed_dim, # Features for query, key, value projections\n",
        "        out_features=self.embed_dim # Output features dimension\n",
        "    )\n",
        "\n",
        "    # c. Create a series of 'd' dense layers, each of size 'x'\n",
        "    self.dense_layers = [nn.Dense(features=self.x_dense_layer_size) for _ in range(self.d_dense_layers)]\n",
        "\n",
        "    # d. Define a final unembedding layer\n",
        "    # This layer projects to 'self.num_embeddings' for classification/prediction of the next number.\n",
        "    self.unembedding_layer = nn.Dense(features=self.num_embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b083a80",
        "outputId": "cd4069e7-3581-45a0-f142-73590600f429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grok model initialized successfully!\n",
            "Initialized parameters structure:\n",
            "{'params': {'attention_block': {'key': {'bias': (4, 8), 'kernel': (32, 4, 8)}, 'out': {'bias': (32,), 'kernel': (4, 8, 32)}, 'query': {'bias': (4, 8), 'kernel': (32, 4, 8)}, 'value': {'bias': (4, 8), 'kernel': (32, 4, 8)}}, 'dense_layers_0': {'bias': (512,), 'kernel': (32, 512)}, 'dense_layers_1': {'bias': (512,), 'kernel': (512, 512)}, 'dense_layers_2': {'bias': (512,), 'kernel': (512, 512)}, 'embedding_layer': {'embedding': (101, 32)}, 'unembedding_layer': {'bias': (101,), 'kernel': (512, 101)}}}\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# Define the model parameters as specified by the user\n",
        "n_attention_heads = 4\n",
        "d_dense_layers = 3\n",
        "x_dense_layer_size = 512\n",
        "num_embeddings = 101 # User specified 100, original was 101 for 0-100\n",
        "embed_dim = 32\n",
        "\n",
        "# Create an instance of the Grok model\n",
        "grok_model = Grok(\n",
        "    n_attention_heads=n_attention_heads,\n",
        "    d_dense_layers=d_dense_layers,\n",
        "    x_dense_layer_size=x_dense_layer_size,\n",
        "    num_embeddings=num_embeddings,\n",
        "    embed_dim=embed_dim\n",
        ")\n",
        "\n",
        "# Generate a dummy input for initialization. The model expects (batch_size, 2)\n",
        "# Let's assume a batch size of 1 and two input numbers (indices).\n",
        "# Since num_embeddings is 100, valid indices are 0 to 99.\n",
        "dummy_input = jnp.array([[10, 20]], dtype=jnp.int32)\n",
        "\n",
        "# Initialize the model's parameters\n",
        "key = jax.random.PRNGKey(1234)\n",
        "params = grok_model.init(key, dummy_input)\n",
        "\n",
        "print(\"Grok model initialized successfully!\")\n",
        "print(\"Initialized parameters structure:\")\n",
        "print(jax.tree.map(lambda x: x.shape, params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fa984f4",
        "outputId": "cd98c307-a5d6-49e8-e5c4-a173be977326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random input numbers: [[88 75]\n",
            " [ 3 70]\n",
            " [94 28]\n",
            " [58  5]\n",
            " [ 2 54]\n",
            " [58 46]\n",
            " [ 4 25]\n",
            " [31 62]\n",
            " [56 87]\n",
            " [33 48]]\n",
            "Model predicted output: [67 79  5 11 11 11 67 11 67 21]\n"
          ]
        }
      ],
      "source": [
        "# Generate a batch of random input numbers (indices)\n",
        "# Ensure the random numbers are within the valid range [0, num_embeddings - 1]\n",
        "random_key, subkey = jax.random.split(key)\n",
        "random_inputs = jax.random.randint(subkey, (10, 2), minval=0, maxval=num_embeddings, dtype=jnp.int32)\n",
        "\n",
        "print(f\"Random input numbers: {random_inputs}\")\n",
        "\n",
        "# Run the model with the initialized parameters and the random inputs\n",
        "logits = grok_model.apply(params, random_inputs)\n",
        "probabilities = jax.nn.softmax(logits)\n",
        "\n",
        "print(f\"Model predicted output: {jnp.argmax(probabilities, -1)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "a3faadec"
      },
      "outputs": [],
      "source": [
        "import optax # Optimization library for JAX\n",
        "\n",
        "# 1. Define the loss function\n",
        "# For classification tasks with integer labels, softmax_cross_entropy_with_integer_labels is suitable.\n",
        "# It expects logits (model output before softmax) and integer labels.\n",
        "# The function takes `logits` and `labels` as input and returns the per-example loss.\n",
        "def cross_entropy_loss(logits, labels):\n",
        "    # The optax function expects labels to be integers representing class indices\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=labels)\n",
        "    return jnp.mean(loss) # Return the mean loss over the batch\n",
        "\n",
        "# 2. Define the optimizer\n",
        "# AdamW is a popular choice for deep learning models.\n",
        "learning_rate = 1e-3 # A common starting learning rate\n",
        "optimizer = optax.adamw(learning_rate=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "167ab1ed",
        "outputId": "8b3dac35-774d-4a41-8b03-31d046ba86cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full dataset generated with 10201 samples (all 101x101 combinations).\n",
            "Full inputs shape: (10201, 2), dtype: int32\n",
            "Full targets shape: (10201,), dtype: int32\n",
            "\n",
            "Dataset split: 6120 training samples, 4081 test samples.\n",
            "First 5 training input-target pairs:\n",
            "  Input: [98 24], Target: 21\n",
            "  Input: [45 71], Target: 15\n",
            "  Input: [100  69], Target: 68\n",
            "  Input: [25 96], Target: 20\n",
            "  Input: [70 53], Target: 22\n",
            "\n",
            "First 5 test input-target pairs:\n",
            "  Input: [28 80], Target: 7\n",
            "  Input: [43 15], Target: 58\n",
            "  Input: [47 99], Target: 45\n",
            "  Input: [27 29], Target: 56\n",
            "  Input: [63  6], Target: 69\n"
          ]
        }
      ],
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "# 3. Generate a dataset with all possible combinations\n",
        "# Inputs x and y are numbers from 0 to num_embeddings-1\n",
        "# Output is (x + y) % num_embeddings\n",
        "\n",
        "def generate_full_dataset(num_embeddings):\n",
        "    # Generate all possible values for x and y\n",
        "    x_values = jnp.arange(num_embeddings, dtype=jnp.int32)\n",
        "    y_values = jnp.arange(num_embeddings, dtype=jnp.int32)\n",
        "\n",
        "    # Create all possible pairs (x, y)\n",
        "    # jnp.meshgrid with indexing='ij' ensures (x,y) pairs are generated column by column for X then row by row for Y\n",
        "    X, Y = jnp.meshgrid(x_values, y_values, indexing='ij')\n",
        "    inputs = jnp.stack([X.flatten(), Y.flatten()], axis=1) # shape (num_embeddings^2, 2)\n",
        "\n",
        "    # Calculate targets: (x + y) % num_embeddings\n",
        "    targets = jnp.sum(inputs, axis=1) % num_embeddings\n",
        "\n",
        "    # Sort the combinations: first by x, then by y\n",
        "    # jnp.lexsort sorts by multiple keys, with the last key being primary.\n",
        "    # So, we want to sort by inputs[:, 0] (x) first, then inputs[:, 1] (y).\n",
        "    # The keys for lexsort are provided in reverse order of precedence.\n",
        "    sort_indices = jnp.lexsort((inputs[:, 1], inputs[:, 0]))\n",
        "    sorted_inputs = inputs[sort_indices]\n",
        "    sorted_targets = targets[sort_indices]\n",
        "\n",
        "    return sorted_inputs, sorted_targets\n",
        "\n",
        "# Set parameters for the dataset generation\n",
        "# num_embeddings is already defined globally from previous cells (e.g., 100)\n",
        "\n",
        "# Generate the full dataset\n",
        "full_inputs, full_targets = generate_full_dataset(num_embeddings)\n",
        "\n",
        "print(f\"Full dataset generated with {full_inputs.shape[0]} samples (all {num_embeddings}x{num_embeddings} combinations).\")\n",
        "print(f\"Full inputs shape: {full_inputs.shape}, dtype: {full_inputs.dtype}\")\n",
        "print(f\"Full targets shape: {full_targets.shape}, dtype: {full_targets.dtype}\")\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_ratio = 0.6\n",
        "num_total_samples = full_inputs.shape[0]\n",
        "num_train_samples = int(num_total_samples * train_ratio)\n",
        "\n",
        "# Shuffle indices to ensure a random split, even though the data is sorted\n",
        "# Use a fixed key for reproducibility\n",
        "key, _ = jax.random.split(jax.random.PRNGKey(42)) # Re-initialize key for this step if needed\n",
        "shuffled_indices = jax.random.permutation(key, num_total_samples)\n",
        "\n",
        "train_indices = shuffled_indices[:num_train_samples]\n",
        "test_indices = shuffled_indices[num_train_samples:]\n",
        "\n",
        "train_inputs = full_inputs[train_indices] # Renaming to dummy_inputs for consistency with train_model signature\n",
        "train_targets = full_targets[train_indices]\n",
        "\n",
        "test_inputs = full_inputs[test_indices]\n",
        "test_targets = full_targets[test_indices]\n",
        "\n",
        "print(f\"\\nDataset split: {train_inputs.shape[0]} training samples, {test_inputs.shape[0]} test samples.\")\n",
        "print(\"First 5 training input-target pairs:\")\n",
        "for i in range(5):\n",
        "    print(f\"  Input: {train_inputs[i]}, Target: {train_targets[i]}\")\n",
        "\n",
        "print(\"\\nFirst 5 test input-target pairs:\")\n",
        "for i in range(5):\n",
        "    print(f\"  Input: {test_inputs[i]}, Target: {test_targets[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5c8d423",
        "outputId": "2b0eb551-1c30-4341-c958-fe6b57773d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 'train_step' function has been defined and compiled with JIT.\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "\n",
        "# The Grok model instance and cross_entropy_loss function are already defined in previous cells.\n",
        "# We need to make sure `grok_model` and `optimizer` are accessible in this scope.\n",
        "# For the purpose of this cell, we assume they are globally available from previous executions.\n",
        "\n",
        "@jax.jit # Compile the function for performance\n",
        "def train_step(params, opt_state, batch_inputs, batch_labels):\n",
        "    # Define the loss function to be differentiated\n",
        "    def loss_fn(current_params):\n",
        "        # Get logits from the model\n",
        "        logits = grok_model.apply(current_params, batch_inputs)\n",
        "        # Calculate the loss using the predefined cross_entropy_loss\n",
        "        loss = cross_entropy_loss(logits, batch_labels)\n",
        "        return loss\n",
        "\n",
        "    # Compute the loss and gradients using jax.value_and_grad\n",
        "    loss_value, grads = jax.value_and_grad(loss_fn)(params)\n",
        "\n",
        "    # Update the optimizer's state and get parameter updates\n",
        "    updates, new_opt_state = optimizer.update(grads, opt_state, params)\n",
        "\n",
        "    # Apply the updates to the model parameters\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "\n",
        "    return new_params, new_opt_state, loss_value\n",
        "\n",
        "print(\"The 'train_step' function has been defined and compiled with JIT.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6fdeca2",
        "outputId": "644c75cb-ec3c-4428-e146-8f586a366e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer state initialized successfully!\n",
            "Type of opt_state: <class 'tuple'>\n",
            "First few elements of opt_state:\n",
            "(ScaleByAdamState(count=(), mu={'params': {'attention_block': {'key': {'bias': (4, 8), 'kernel': (32, 4, 8)}, 'out': {'bias': (32,), 'kernel': (4, 8, 32)}, 'query': {'bias': (4, 8), 'kernel': (32, 4, 8)}, 'value': {'bias': (4, 8), 'kernel': (32, 4, 8)}}, 'dense_layers_0': {'bias': (512,), 'kernel': (32, 512)}, 'dense_layers_1': {'bias': (512,), 'kernel': (512, 512)}, 'dense_layers_2': {'bias': (512,), 'kernel': (512, 512)}, 'embedding_layer': {'embedding': (101, 32)}, 'unembedding_layer': {'bias': (101,), 'kernel': (512, 101)}}}, nu={'params': {'attention_block': {'key': {'bias': (4, 8), 'kernel': (32, 4, 8)}, 'out': {'bias': (32,), 'kernel': (4, 8, 32)}, 'query': {'bias': (4, 8), 'kernel': (32, 4, 8)}, 'value': {'bias': (4, 8), 'kernel': (32, 4, 8)}}, 'dense_layers_0': {'bias': (512,), 'kernel': (32, 512)}, 'dense_layers_1': {'bias': (512,), 'kernel': (512, 512)}, 'dense_layers_2': {'bias': (512,), 'kernel': (512, 512)}, 'embedding_layer': {'embedding': (101, 32)}, 'unembedding_layer': {'bias': (101,), 'kernel': (512, 101)}}}), EmptyState(), EmptyState())\n"
          ]
        }
      ],
      "source": [
        "opt_state = optimizer.init(params)\n",
        "\n",
        "print(\"Optimizer state initialized successfully!\")\n",
        "print(f\"Type of opt_state: {type(opt_state)}\")\n",
        "print(\"First few elements of opt_state:\")\n",
        "print(jax.tree.map(lambda x: x.shape if hasattr(x, 'shape') else x, opt_state))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50fb78e2"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully initialized the optimizer state. Now, I will implement the `train_model` function that encapsulates the full training loop as requested by the main task. This function will iterate for a specified number of epochs, process the dummy dataset in batches, call the `train_step` function, and collect loss values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "09dacb95"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy as np # For data shuffling\n",
        "import time # Import time module for step time calculation\n",
        "\n",
        "# Helper function to calculate loss and accuracy\n",
        "def calculate_metrics(params, inputs, targets):\n",
        "    logits = grok_model.apply(params, inputs)\n",
        "    loss = cross_entropy_loss(logits, targets)\n",
        "    predictions = jnp.argmax(logits, axis=-1)\n",
        "    accuracy = jnp.mean(predictions == targets)\n",
        "    return loss, accuracy\n",
        "\n",
        "def train_model(initial_params, initial_opt_state, train_inputs, train_targets, test_inputs, test_targets, num_epochs, batch_size, accuracy_threshold=0.8, random_seed=0):\n",
        "    current_params = initial_params\n",
        "    current_opt_state = initial_opt_state\n",
        "    loss_history = []\n",
        "    train_accuracy_history = []\n",
        "    test_accuracy_history = []\n",
        "    avg_step_time_history = [] # To store average step time per epoch\n",
        "\n",
        "    key = jax.random.PRNGKey(random_seed)\n",
        "\n",
        "    num_train_samples = train_inputs.shape[0]\n",
        "    num_train_batches = num_train_samples // batch_size\n",
        "\n",
        "    print(f\"Starting training for {num_epochs} epochs with batch size {batch_size}...\")\n",
        "    print(f\"Early stopping if test accuracy reaches {accuracy_threshold}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        key, subkey = jax.random.split(key)\n",
        "        # Shuffle indices for each epoch\n",
        "        shuffled_indices = jax.random.permutation(subkey, num_train_samples)\n",
        "        shuffled_train_inputs = train_inputs[shuffled_indices]\n",
        "        shuffled_train_targets = train_targets[shuffled_indices]\n",
        "\n",
        "        epoch_losses = []\n",
        "        epoch_step_times = [] # To store step times for the current epoch\n",
        "\n",
        "        for i in range(num_train_batches):\n",
        "            batch_start = i * batch_size\n",
        "            batch_end = (i + 1) * batch_size\n",
        "            batch_inputs = shuffled_train_inputs[batch_start:batch_end]\n",
        "            batch_labels = shuffled_train_targets[batch_start:batch_end]\n",
        "\n",
        "            # Perform a training step and measure its time\n",
        "            start_step_time = time.time()\n",
        "            current_params, current_opt_state, loss_value = train_step(\n",
        "                current_params, current_opt_state, batch_inputs, batch_labels\n",
        "            )\n",
        "            end_step_time = time.time()\n",
        "            epoch_step_times.append(end_step_time - start_step_time)\n",
        "\n",
        "            epoch_losses.append(loss_value)\n",
        "            loss_history.append(loss_value)\n",
        "\n",
        "        key, subkey_train, subkey_test = jax.random.split(key, 3)\n",
        "        # Sample 1000 random indices for Train set\n",
        "        train_sample_idx = jax.random.choice(subkey_train, num_train_samples, shape=(1000,), replace=False)\n",
        "        sample_train_inputs = train_inputs[train_sample_idx]\n",
        "        sample_train_targets = train_targets[train_sample_idx]\n",
        "\n",
        "        # Sample 1000 random indices for Test set (handling cases where test set < 1000)\n",
        "        num_test_samples = test_inputs.shape[0]\n",
        "        test_sample_size = min(1000, num_test_samples)\n",
        "        test_sample_idx = jax.random.choice(subkey_test, num_test_samples, shape=(test_sample_size,), replace=False)\n",
        "        sample_test_inputs = test_inputs[test_sample_idx]\n",
        "        sample_test_targets = test_targets[test_sample_idx]\n",
        "        # Calculate metrics on the subsets\n",
        "        _, train_accuracy = calculate_metrics(current_params, sample_train_inputs, sample_train_targets)\n",
        "        _, test_accuracy = calculate_metrics(current_params, sample_test_inputs, sample_test_targets)\n",
        "        # Evaluate metrics on full train and test sets after each epoch\n",
        "        train_loss, train_accuracy = calculate_metrics(current_params, train_inputs, train_targets)\n",
        "        test_loss, test_accuracy = calculate_metrics(current_params, test_inputs, test_targets)\n",
        "\n",
        "        train_accuracy_history.append(train_accuracy)\n",
        "        test_accuracy_history.append(test_accuracy)\n",
        "\n",
        "        # Calculate average step time for the epoch\n",
        "        avg_epoch_step_time = jnp.mean(jnp.array(epoch_step_times))\n",
        "        avg_step_time_history.append(avg_epoch_step_time)\n",
        "\n",
        "        # Print average loss and accuracy for the epoch\n",
        "        avg_epoch_loss = jnp.mean(jnp.array(epoch_losses))\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Avg Train Loss: {avg_epoch_loss:.32f}, Train Acc: {train_accuracy:.4f}, Test Acc: {test_accuracy:.4f}, Avg Step Time: {avg_epoch_step_time:.4f}s\")\n",
        "\n",
        "        # Early stopping condition\n",
        "        if test_accuracy >= accuracy_threshold:\n",
        "            print(f\"Early stopping triggered: Test accuracy {test_accuracy:.4f} >= {accuracy_threshold:.4f}\")\n",
        "            break\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    return current_params, current_opt_state, loss_history, train_accuracy_history, test_accuracy_history, avg_step_time_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33f51d41",
        "outputId": "808731e9-ac1c-4f15-f07c-637af1a1b95a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined batch sizes to test: [1, 2, 4, 8, 16, 32, 64, 128]\n"
          ]
        }
      ],
      "source": [
        "batch_sizes_to_test = [1, 2, 4, 8, 16, 32, 64, 128]\n",
        "num_epochs = 30\n",
        "accuracy_threshold = 1\n",
        "\n",
        "print(f\"Defined batch sizes to test: {batch_sizes_to_test}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "37d93a80",
        "outputId": "64be4db2-e19f-4193-9ad0-8a4efb29e2d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training iterations for different batch sizes...\n",
            "\n",
            "----------------------------------------\n",
            "Training with batch_size = 1\n",
            "----------------------------------------\n",
            "Starting training for 30 epochs with batch size 1...\n",
            "Early stopping if test accuracy reaches 1\n",
            "Epoch 1/30, Avg Train Loss: 4.62979793548583984375000000000000, Train Acc: 0.0118, Test Acc: 0.0071, Avg Step Time: 0.0013s\n",
            "Epoch 2/30, Avg Train Loss: 4.62303113937377929687500000000000, Train Acc: 0.0118, Test Acc: 0.0066, Avg Step Time: 0.0010s\n",
            "Epoch 3/30, Avg Train Loss: 4.62077379226684570312500000000000, Train Acc: 0.0119, Test Acc: 0.0069, Avg Step Time: 0.0010s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-573705329.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mkey_initial_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubkey_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_initial_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     _, _, loss_history, _, test_acc_history, _ = train_model(\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mnew_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_opt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubkey_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use subkey as random_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1629760945.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(initial_params, initial_opt_state, train_inputs, train_targets, test_inputs, test_targets, num_epochs, batch_size, accuracy_threshold, random_seed)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mcurrent_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_opt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             )\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mend_step_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mepoch_step_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_step_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "final_accuracies = {}\n",
        "all_loss_history = {}\n",
        "\n",
        "# Re-initialize the model and optimizer for each run. Need initial_params and initial_opt_state\n",
        "# The original 'params' and 'opt_state' were initialized in previous cells.\n",
        "# We need to make sure we start from a fresh model for each batch size.\n",
        "\n",
        "# Function to re-initialize model parameters and optimizer state\n",
        "def reinitialize_model_and_optimizer(key, dummy_input, grok_model, optimizer):\n",
        "    new_params = grok_model.init(key, dummy_input)\n",
        "    new_opt_state = optimizer.init(new_params)\n",
        "    return new_params, new_opt_state\n",
        "\n",
        "\n",
        "print(\"Starting training iterations for different batch sizes...\")\n",
        "key_initial_params = jax.random.PRNGKey(0) # Use a fixed key for reproducible initial params\n",
        "\n",
        "for bs in batch_sizes_to_test:\n",
        "    print(f\"\\n----------------------------------------\")\n",
        "    print(f\"Training with batch_size = {bs}\")\n",
        "    print(f\"----------------------------------------\")\n",
        "\n",
        "    # Re-initialize parameters and optimizer state for the current batch size\n",
        "    # Make sure to use a new subkey for initialization each time if needed,\n",
        "    # but for overall reproducibility, using the same initial key for params creation is fine.\n",
        "    # The training key for shuffling inside train_model will be different anyway.\n",
        "\n",
        "    # Re-using the dummy_input defined earlier\n",
        "    new_params, new_opt_state = reinitialize_model_and_optimizer(key_initial_params, dummy_input, grok_model, optimizer)\n",
        "\n",
        "    # Train the model with the current batch size\n",
        "    # Use a fresh random seed for train_model for each batch size run\n",
        "    key_initial_params, subkey_train = jax.random.split(key_initial_params)\n",
        "\n",
        "    _, _, loss_history, _, test_acc_history, _ = train_model(\n",
        "        new_params, new_opt_state, train_inputs, train_targets, test_inputs, test_targets,\n",
        "        num_epochs, bs, accuracy_threshold, random_seed=subkey_train[0].item() # Use subkey as random_seed\n",
        "    )\n",
        "\n",
        "    final_accuracies[bs] = test_acc_history[-1]\n",
        "    all_loss_history[bs] = loss_history\n",
        "    print(f\"Final test accuracy for batch_size {bs}: {final_accuracies[bs]:.4f}\")\n",
        "\n",
        "print(\"\\n----------------------------------------\")\n",
        "print(\"Training iterations complete!\")\n",
        "print(\"Summary of final test accuracies:\")\n",
        "for bs, acc in final_accuracies.items():\n",
        "    print(f\"Batch Size: {bs}, Final Test Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdd3d67b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Retrieve num_train_samples from the kernel state, which was defined in cell 167ab1ed.\n",
        "num_train_samples = train_inputs.shape[0]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Iterate through each batch size and its corresponding flat loss history\n",
        "for bs, loss_history_flat in all_loss_history.items():\n",
        "    # Calculate the number of batches per epoch for the current batch size\n",
        "    num_train_batches = num_train_samples // bs\n",
        "\n",
        "    # Skip if batch size is larger than training samples, as no training would occur\n",
        "    if num_train_batches == 0:\n",
        "        print(f\"Warning: Batch size {bs} is larger than num_train_samples {num_train_samples}. Skipping plot for this batch size.\")\n",
        "        continue\n",
        "\n",
        "    # Determine the actual number of epochs that were run for this batch size\n",
        "    # This is inferred from the total length of the loss history divided by batches per epoch.\n",
        "    actual_epochs_run = len(loss_history_flat) // num_train_batches\n",
        "\n",
        "    # Calculate the average loss for each epoch\n",
        "    epoch_losses = []\n",
        "    for i in range(actual_epochs_run):\n",
        "        batch_losses_in_epoch = loss_history_flat[i * num_train_batches : (i + 1) * num_train_batches]\n",
        "        if batch_losses_in_epoch: # Ensure there are losses to average\n",
        "            epoch_losses.append(np.mean(batch_losses_in_epoch))\n",
        "        else:\n",
        "            # This case should ideally not happen if actual_epochs_run is calculated correctly.\n",
        "            print(f\"Warning: No batch losses found for epoch {i+1} for batch size {bs}. Stopping processing for this batch size.\")\n",
        "            break\n",
        "\n",
        "    if epoch_losses: # Only plot if there are valid epoch losses\n",
        "        # Convert epoch losses to a numpy array and apply logarithm.\n",
        "        # A small epsilon is added to avoid issues with log(0) if losses become very small.\n",
        "        log_epoch_losses = np.log(np.array(epoch_losses) + 1e-9)\n",
        "\n",
        "        # Plot the log of average epoch loss against the epoch number\n",
        "        plt.plot(range(1, actual_epochs_run + 1), log_epoch_losses, label=f'Batch Size: {bs}')\n",
        "    else:\n",
        "        print(f\"No valid epoch losses to plot for batch size {bs}.\")\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Log(Average Training Loss)')\n",
        "plt.title('Logarithm of Average Training Loss per Epoch for Different Batch Sizes')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Plot of log of average training loss per epoch for different batch sizes generated.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5997750"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure final_accuracies is populated (assuming previous cell completes execution)\n",
        "# For demonstration, if previous cell output is truncated, we might have to manually define it for plotting\n",
        "# if not final_accuracies:\n",
        "#     # This is a fallback if the previous cell didn't fully execute for ALL batch sizes.\n",
        "#     # In a real scenario, the previous cell would complete.\n",
        "#     print(\"Warning: final_accuracies not fully populated. Using dummy data for plot.\")\n",
        "#     final_accuracies = {\n",
        "#         2: 0.0047,\n",
        "#         4: 0.0050,\n",
        "#         8: 0.0120,\n",
        "#         16: 0.5000,\n",
        "#         32: 0.9500,\n",
        "#         64: 0.9966,\n",
        "#         128: 0.9970,\n",
        "#         256: 0.9972\n",
        "#     }\n",
        "\n",
        "# Convert results to lists for plotting\n",
        "batch_sizes = list(final_accuracies.keys())\n",
        "accuracies = list(final_accuracies.values())\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(batch_sizes, accuracies, marker='o', linestyle='-', color='skyblue')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xscale('log', base=2) # Batch sizes are powers of 2, so a log scale is appropriate\n",
        "plt.xticks(batch_sizes, labels=[str(bs) for bs in batch_sizes])\n",
        "plt.xlabel('Batch Size (log scale)')\n",
        "plt.ylabel('Final Test Accuracy')\n",
        "plt.title('Impact of Batch Size on Final Test Accuracy')\n",
        "plt.grid(True, which=\"both\", ls=\"--\", c='0.7')\n",
        "plt.ylim(0, 1.05) # Accuracy is between 0 and 1\n",
        "\n",
        "# Add text labels for each point\n",
        "for i, txt in enumerate(accuracies):\n",
        "    plt.annotate(f'{txt:.4f}', (batch_sizes[i], accuracies[i]), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Plot of final test accuracy vs. batch size generated.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Testing Output: [link](https://pastebin.com/f2VncuYV)"
      ],
      "metadata": {
        "id": "Bg1X2fCsHJbj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oLUZie9jHynL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "authorship_tag": "ABX9TyOQc2RbZSOF49BbJ0hqSAx3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}